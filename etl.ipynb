{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¦„ğŸ¦„ğŸ¦„       ğŸ¦„ğŸ¦„ğŸ¦„     ğŸ¦„ğŸ¦„ğŸ¦„      ğŸ¦„ğŸ¦„ğŸ¦„      ğŸ¦„ğŸ¦„ğŸ¦„      ğŸ¦„ğŸ¦„ğŸ¦„     ğŸ¦„ğŸ¦„ğŸ¦„    ğŸ¦„ğŸ¦„ğŸ¦„    ğŸ¦„ğŸ¦„ğŸ¦„   ğŸ¦„ğŸ¦„ğŸ¦„    ğŸ¦„ğŸ¦„ğŸ¦„     ğŸ¦„ğŸ¦„ğŸ¦„ ğŸ¦„ğŸ¦„ğŸ¦„ ğŸ¦„ğŸ¦„ğŸ¦„ ğŸ¦„ğŸ¦„ğŸ¦„ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome to the ETL task for getting Unicorn.SalesğŸ¦„ Business to New heights.**\n",
    "\n",
    "In this notebook, we will carry out following *ETL Tasks*:\n",
    "\n",
    "1. *Download Data: Get the Data from prepopulated Aurora Database from your account.*  \n",
    "2. *Data Cleaning: Clean the data, add/delete/rename columns in the data downloaded in step 1.*\n",
    "3. *Data Upload: Upload these cleaned csv's to S3 bucket.*\n",
    "\n",
    "As a player, you will need to fill in values or modify the code wherever it is noted as `PlayerInputRequired` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: **Downloadig Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will: \n",
    "- [ ] Install Dependencies\n",
    "- [ ] Connect to database \n",
    "- [ ] Query database tables for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Install Dependencies**  \n",
    "[Takes about 5 mins] \n",
    "\n",
    "`psycopg2` is a python package to set up connection with rds database which holds the data. As psycopg2 has multiple depencies which may conflict with other packages when installed using pip, we will installing this package using conda install. Conda install handles dependcies better given the numerous conda environments deployed by sagemaker studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Connect to Database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  *To set up connection with database, we will need following database credentials. From* [AWS secret manager](https://console.aws.amazon.com/secretsmanager/), *look up hostname(endpoint), region and password. We will initialize all the credentials/params in the following cell.*\n",
    "  \n",
    "Following is an **example**: \n",
    " \n",
    "```\n",
    "host=\"gameday-aurora-postgres.cluster-cj8rrrrrkb.us-east-2.rds.amazonaws.com\"       <- db endpoint  \n",
    "PORT=\"5432\"                                                                         <- default\n",
    "USER=\"postgres\"                                                                     <- default\n",
    "DBNAME=\"postgres\"                                                                   <- DBName \n",
    "password = \"PASSWORD\"                                                               <- database password  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "host=\"ENDPOINT_PlayerInputRequired\"                         # PlayerInputRequired                           \n",
    "PORT=\"5432\"\n",
    "USER=\"postgres\"\n",
    "DBNAME=\"unicorn_sales\"\n",
    "password = \"dbPassword_PlayerInputRequired\"                 # PlayerInputRequired "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Destination bucket` to upload cleaned data. Look up your [stack output](https://console.aws.amazon.com/cloudformation) to get bucket name from key `PersonalizeBucketName`* \n",
    "- Select the stack\n",
    "- Click on Output tab to view all the outputs of your deployed stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_s3Bucket = 'S3_BUCKET_NAME_PlayerInputRequired'   # PlayerInputRequired "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import python packages and set up connection using connect method of* `psycopg2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(host = host, \n",
    "                        database = DBNAME, \n",
    "                        user = USER, \n",
    "                        password = password)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Query database tables for the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modify following queries to select desired columns from database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itens_df.head()   # print the dataframe to view all the column names and select desired one in following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df=pd.read_sql(\"\"\"SELECT *  FROM gameday.product\"\"\", conn)     # PlayerInputRequired "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.head()   # print the dataframe to view all the column names and select desired one in following query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df=pd.read_sql(\"\"\"SELECT * FROM gameday.customer \"\"\", conn)    # PlayerInputRequired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head().  # print the dataframe to view all the column names and select desired one in following query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df=pd.read_sql(\"\"\"SELECT * FROM gameday.internetsales\"\"\", conn)  # PlayerInputRequired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original interactions as it will be referenced couple of time after cleaning\n",
    "interactions_df_orig = interactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `interactions_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# filter unique product keys\n",
    "unique_product_keys = set(items_df['productkey'].unique())  \n",
    "\n",
    "# filter interactions data rows where product key exists in unique product key\n",
    "interactions_df = interactions_df[interactions_df['productkey'].isin(unique_product_keys)]  \n",
    "\n",
    "#convert order datetime column datatype to pandas datetime data type\n",
    "interactions_df['OrderDate_datetime'] = pd.to_datetime(interactions_df['orderdate'])\n",
    "\n",
    "#add column TIMESTAMP which contains above OrderDate time converted in numpy int64 datatype\n",
    "interactions_df['TIMESTAMP'] = interactions_df.OrderDate_datetime.values.astype(np.int64) // 10 ** 9\n",
    " \n",
    "# column rename column 'productkey' to 'ITEM_ID' and column 'customerkey' to 'USER_ID'\n",
    "interactions_df = interactions_df.rename(columns={'productkey':'ITEM_ID', 'customerkey':'USER_ID'}) \n",
    "\n",
    "# select columns ITEM_ID, USER_ID and TIMESTAMP only from interactions dataframe and drop other columns. \n",
    "interactions_df = interactions_df[['ITEM_ID', 'USER_ID', 'TIMESTAMP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets print how interactions_df dataframe looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save cleaned df to csv in local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os and pathlib to create directory locally\n",
    "import os\n",
    "from pathlib import Path \n",
    "\n",
    "# path where we want to store the cleaned interactions dataframe\n",
    "path = Path(\"./data/clean\")\n",
    "\n",
    "# create the directory structure if it doesnt exist\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save the dataframe to a csv file named interactions.csv\n",
    "interactions_df.to_csv(os.path.join(path, 'interactions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `users_df` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter unique product keys\n",
    "unique_user_keys = set(interactions_df_orig['customerkey'].unique())\n",
    "\n",
    "# filter users data rows where product key exists in unique product key\n",
    "users_df = users_df[users_df['customerkey'].isin(unique_user_keys)]\n",
    "\n",
    "# column rename column customerkey to USER_ID, 'gender' to 'Gender', 'yearlyincome' to 'YearlyIncome\n",
    "users_df = users_df.rename(columns={'customerkey':'USER_ID', 'gender': 'Gender', 'yearlyincome': 'YearlyIncome'})\n",
    "\n",
    "# save users dataframe locally to users.csv file\n",
    "users_df.to_csv(os.path.join(path, 'users.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets print users_df dataframe to checkout cleaned version \n",
    "users_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean `items_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = items_df[items_df['productsubcategorykey'] < 4]\n",
    "items_df['productsubcategorykey'] = items_df['productsubcategorykey'].replace({1: 'Mountain Cosmic Unicorn', 2: 'Road Cosmic Unicorn', 3: 'Touring Cosmic Unicorn'})\n",
    "items_df = items_df.rename(columns={'productkey':'ITEM_ID', 'productsubcategorykey':'ProductSubcategory', 'listprice': 'ListPrice'})\n",
    "\n",
    "items_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.to_csv('./data/clean/items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Upload data to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous steps, we cleaned the data and saved dataframe into a csv on our local disk. To use this data for training in personalize, lets save it into s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/interactions.csv').upload_file('./data/clean/interactions.csv')\n",
    "\n",
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/items.csv').upload_file('./data/clean/items.csv')\n",
    "\n",
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/users.csv').upload_file('./data/clean/users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the cleaned csv files are on S3, we will be importing these into personlize engine for training in next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional:  Files uploaded to s3 bucket can be checked [here](https://console.aws.amazon.com/s3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congrats!** your have completed ETL task and got data ready for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
