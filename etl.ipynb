{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¦„ðŸ¦„ðŸ¦„       ðŸ¦„ðŸ¦„ðŸ¦„     ðŸ¦„ðŸ¦„ðŸ¦„      ðŸ¦„ðŸ¦„ðŸ¦„      ðŸ¦„ðŸ¦„ðŸ¦„      ðŸ¦„ðŸ¦„ðŸ¦„     ðŸ¦„ðŸ¦„ðŸ¦„    ðŸ¦„ðŸ¦„ðŸ¦„    ðŸ¦„ðŸ¦„ðŸ¦„   ðŸ¦„ðŸ¦„ðŸ¦„    ðŸ¦„ðŸ¦„ðŸ¦„     ðŸ¦„ðŸ¦„ðŸ¦„ ðŸ¦„ðŸ¦„ðŸ¦„ ðŸ¦„ðŸ¦„ðŸ¦„ ðŸ¦„ðŸ¦„ðŸ¦„ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welcome to the ETL task for getting Unicorn.SalesðŸ¦„ Business to New heights.**\n",
    "\n",
    "In this notebook, we will carry out following *ETL Tasks*:\n",
    "\n",
    "1. *Download Data: Get the Data from prepopulated Aurora Database from your account.*  \n",
    "2. *Data Cleaning: Clean the data, add/delete/rename columns in the data downloaded in step 1.*\n",
    "3. *Data Upload: Upload these cleaned csv's to S3 bucket.*\n",
    "\n",
    "As a player, you will need to fill in values or modify the code wherever it is noted as `<PlayerInputRequired>` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: **Downloading Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will: \n",
    "- [ ] Connect to database \n",
    "- [ ] Query database tables for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Connect to Database**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  *To set up connection with database, we will need following database credentials. From* [AWS Secrets Manager](https://console.aws.amazon.com/secretsmanager/), *look up host(endpoint) and password. We will initialize all the credentials/params in the following cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host=\"HOST\"                           # <PlayerInputRequired>                           \n",
    "PORT=\"5432\"                           # default, verify value\n",
    "USER=\"postgres\"                       # pre-populated, verify value\n",
    "DBNAME=\"unicorn_sales\"                # pre-populated, verify value\n",
    "password = \"PASSWORD\"                 # <PlayerInputRequired> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Destination bucket` to upload cleaned data. Look up your [stack output](https://console.aws.amazon.com/cloudformation) to get bucket name from key `PersonalizeBucketName`* \n",
    "- Select the stack\n",
    "- Click on Output tab to view all the outputs of your deployed stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_s3Bucket = 'S3_BUCKET_NAME'   # <PlayerInputRequired> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To executes all fields from here: \n",
    "\n",
    "On Toolbar  **Cell > Run All**\n",
    "\n",
    "Feel free to play with dataframes transformed or peek into it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import python packages and set up connection using connect method of* `psycopg2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(host = host, \n",
    "                        database = DBNAME, \n",
    "                        user = USER, \n",
    "                        password = password)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Querying the database tables for the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Review the following queries which select the desired columns from database. \n",
    "**Note: No user updates are required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query `product` table to get specific columns\"\n",
    "items_df=pd.read_sql(\"\"\"SELECT productkey, productsubcategorykey, listprice FROM gameday.product\"\"\", conn)     \n",
    "items_df.head()   # print the dataframe to view all the column names and select desired one in following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df=pd.read_sql(\"\"\"SELECT customerkey, gender, yearlyincome FROM gameday.customer \"\"\", conn)    \n",
    "users_df.head()   # print the dataframe to view all the column names and select desired one in following query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df=pd.read_sql(\"\"\"SELECT productkey, customerkey, orderdate FROM gameday.internetsales\"\"\", conn) \n",
    "interactions_df.head()  # print the dataframe to view all the column names and select desired one in following query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*fetching dates dataframe to join with internetsales in coming steps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=pd.read_sql(\"\"\"SELECT * FROM gameday.dates\"\"\", conn)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original interactions as it will be referenced couple of time after cleaning\n",
    "interactions_df_orig = pd.read_sql(\"\"\"SELECT * FROM gameday.internetsales\"\"\", conn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the `interactions_df` dataframe.\n",
    "\n",
    "**Note: No user updates are required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# filter unique product keys\n",
    "unique_product_keys = set(items_df['productkey'].unique())  \n",
    "\n",
    "# filter interactions data rows where product key exists in unique product key\n",
    "interactions_df = interactions_df[interactions_df['productkey'].isin(unique_product_keys)]  \n",
    "\n",
    "# join internetsales with dates table on datekey and get the FullDateAlternateKey\n",
    "interactions_df['OrderDate_datetime'] = pd.merge(interactions_df_orig, dates, left_on='orderdatekey', right_on='datekey')['fulldatealternatekey']\n",
    "\n",
    "#convert order datetime column datatype to pandas datetime data type\n",
    "interactions_df['OrderDate_datetime'] = pd.to_datetime(interactions_df['OrderDate_datetime'])\n",
    "\n",
    "#add column TIMESTAMP which contains above OrderDate time converted in numpy int64 datatype\n",
    "interactions_df['TIMESTAMP'] = interactions_df.OrderDate_datetime.values.astype(np.int64) // 10 ** 9\n",
    " \n",
    "# column rename column 'productkey' to 'ITEM_ID' and column 'customerkey' to 'USER_ID'\n",
    "interactions_df = interactions_df.rename(columns={'productkey':'ITEM_ID', 'customerkey':'USER_ID'}) \n",
    "\n",
    "# select columns ITEM_ID, USER_ID and TIMESTAMP only from interactions dataframe and drop other columns. \n",
    "interactions_df = interactions_df[['ITEM_ID', 'USER_ID', 'TIMESTAMP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print how interactions_df dataframe looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save cleaned df to csv in local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os and pathlib to create directory locally\n",
    "import os\n",
    "from pathlib import Path \n",
    "\n",
    "# path where we want to store the cleaned interactions dataframe\n",
    "path = Path(\"./data/clean\")\n",
    "\n",
    "# create the directory structure if it doesnt exist\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save the dataframe to a csv file named interactions.csv\n",
    "interactions_df.to_csv(os.path.join(path, 'interactions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the `users_df` dataframe\n",
    "\n",
    "**Note: No user updates required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter unique product keys\n",
    "unique_user_keys = set(interactions_df_orig['customerkey'].unique())\n",
    "\n",
    "# filter users data rows where product key exists in unique product key\n",
    "users_df = users_df[users_df['customerkey'].isin(unique_user_keys)]\n",
    "\n",
    "# column rename column customerkey to USER_ID, 'gender' to 'Gender', 'yearlyincome' to 'YearlyIncome\n",
    "users_df = users_df.rename(columns={'customerkey':'USER_ID', 'gender': 'Gender', 'yearlyincome': 'YearlyIncome'})\n",
    "\n",
    "# save users dataframe locally to users.csv file\n",
    "users_df.to_csv(os.path.join(path, 'users.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets print users_df dataframe to checkout cleaned version \n",
    "users_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the `items_df` dataframe\n",
    "\n",
    "**Note: No user updates required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df['productsubcategorykey'] = items_df['productsubcategorykey'].replace({1: 'Mountain Cosmic Unicorn', 2: 'Road Cosmic Unicorn', 3: 'Touring Cosmic Unicorn'})\n",
    "items_df = items_df.rename(columns={'productkey':'ITEM_ID', 'productsubcategorykey':'ProductSubcategory', 'listprice': 'ListPrice'})\n",
    "\n",
    "items_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.to_csv('./data/clean/items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Upload data to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous steps, we cleaned the data and saved dataframe into a csv on our local disk. To use this data for training in personalize, lets save it into s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/interactions.csv').upload_file('./data/clean/interactions.csv')\n",
    "\n",
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/items.csv').upload_file('./data/clean/items.csv')\n",
    "\n",
    "#upload to s3 bucket\n",
    "import boto3\n",
    "boto3.Session().resource('s3').Bucket(dest_s3Bucket)\\\n",
    "        .Object('data/users.csv').upload_file('./data/clean/users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the cleaned csv files are on S3, we will be importing these into personlize engine for training in next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional:  Files uploaded to s3 bucket can be checked [here](https://console.aws.amazon.com/s3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congrats!** your have completed ETL task and got data ready for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8ae706c39141c58ad6b8a2deef872a1c6146ff482198834acdf315491db80214"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
